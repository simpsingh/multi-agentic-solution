# =============================================================================
# Multi-Agentic Solution - Docker Compose Configuration
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL Database
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: agentic-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-agentic_db}
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/init-db.sql:ro
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # OpenSearch Vector Database
  # ---------------------------------------------------------------------------
  opensearch:
    image: opensearchproject/opensearch:2.11.1
    container_name: agentic-opensearch
    environment:
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
      - DISABLE_SECURITY_PLUGIN=true
      - DISABLE_INSTALL_DEMO_CONFIG=true
    ports:
      - "9200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Redis Cache
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: agentic-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Apache Airflow Webserver
  # ---------------------------------------------------------------------------
  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: agentic-airflow-webserver
    command: ["airflow", "webserver"]
    environment:
      # Core Airflow settings
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__PLUGINS_FOLDER=/opt/airflow/plugins

      # Database connection (using separate database for Airflow)
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/airflow_db

      # Webserver settings
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY:-secret_key}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true

      # API settings - basic auth for Airflow 2.9
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth

      # Admin user
      - AIRFLOW_ADMIN_USER=${AIRFLOW_ADMIN_USER:-admin}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}
      - AIRFLOW_ADMIN_EMAIL=${AIRFLOW_ADMIN_EMAIL:-admin@example.com}

      # Application Database (for DAG code to access agentic_db)
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-agentic_db}

      # AWS credentials (for S3 access)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_REGION:-us-east-1}
      - AWS_REGION=${AWS_REGION:-us-east-1}

      # S3 configuration
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_INPUT_PREFIX=${S3_INPUT_PREFIX:-input/}

      # Bedrock (for document parsing with Claude)
      - BEDROCK_MODEL_ID=${BEDROCK_MODEL_ID:-anthropic.claude-sonnet-4-5-20250929-v1:0}
      - BEDROCK_REGION=${BEDROCK_REGION:-us-east-1}

      # PostgreSQL connection for entrypoint
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
    ports:
      - "${AIRFLOW_PORT:-8080}:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    networks:
      - app-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/api/v2/monitor/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Apache Airflow Scheduler
  # ---------------------------------------------------------------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: agentic-airflow-scheduler
    command: ["airflow", "scheduler"]
    environment:
      # Core Airflow settings
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__CORE__PLUGINS_FOLDER=/opt/airflow/plugins

      # Database connection (using separate database for Airflow)
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/airflow_db

      # Admin user (for entrypoint script)
      - AIRFLOW_ADMIN_USER=${AIRFLOW_ADMIN_USER:-admin}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}
      - AIRFLOW_ADMIN_EMAIL=${AIRFLOW_ADMIN_EMAIL:-admin@example.com}

      # Application Database (for DAG code to access agentic_db)
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-agentic_db}

      # AWS credentials (for S3 access)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_REGION:-us-east-1}
      - AWS_REGION=${AWS_REGION:-us-east-1}

      # S3 configuration
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_INPUT_PREFIX=${S3_INPUT_PREFIX:-input/}

      # Bedrock (for document parsing with Claude)
      - BEDROCK_MODEL_ID=${BEDROCK_MODEL_ID:-anthropic.claude-sonnet-4-5-20250929-v1:0}
      - BEDROCK_REGION=${BEDROCK_REGION:-us-east-1}

      # PostgreSQL connection for entrypoint
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    networks:
      - app-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # FastAPI Backend
  # ---------------------------------------------------------------------------
  fastapi:
    build:
      context: .
      dockerfile: src/Dockerfile
    container_name: agentic-fastapi
    environment:
      # Application settings
      - APP_NAME=${APP_NAME:-agentic-solution}
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}

      # Database
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-agentic_db}
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}

      # Redis
      - REDIS_URL=redis://redis:6379/0
      - REDIS_POOL_SIZE=${REDIS_POOL_SIZE:-10}

      # OpenSearch
      - OPENSEARCH_URL=http://opensearch:9200
      - OPENSEARCH_DDL_INDEX=${OPENSEARCH_DDL_INDEX:-ddl_index}
      - OPENSEARCH_SYNTHETIC_INDEX=${OPENSEARCH_TESTDATA_INDEX:-testdata_index}
      - OPENSEARCH_MAX_RETRIES=${OPENSEARCH_MAX_RETRIES:-5}

      # AWS credentials
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-east-1}

      # S3
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_INPUT_PREFIX=${S3_INPUT_PREFIX:-input/}
      - S3_OUTPUT_PREFIX=${S3_OUTPUT_PREFIX:-output/}

      # Bedrock
      - BEDROCK_MODEL_ID=${BEDROCK_MODEL_ID:-anthropic.claude-sonnet-4-5-20250929-v1:0}
      - BEDROCK_REGION=${BEDROCK_REGION:-us-east-1}
      - BEDROCK_STREAMING_ENABLED=${BEDROCK_STREAMING_ENABLED:-true}
      - BEDROCK_MAX_CONCURRENT_REQUESTS=${BEDROCK_MAX_CONCURRENT_REQUESTS:-3}
      - BEDROCK_TIMEOUT=${BEDROCK_TIMEOUT:-60}

      # Bedrock Titan Embeddings
      - BEDROCK_EMBEDDING_MODEL_ID=${BEDROCK_EMBEDDING_MODEL_ID:-amazon.titan-embed-text-v1}
      - BEDROCK_EMBEDDING_DIMENSION=${BEDROCK_EMBEDDING_DIMENSION:-1536}

      # Airflow
      - AIRFLOW_URL=http://airflow-webserver:8080
      - AIRFLOW_ADMIN_USER=${AIRFLOW_ADMIN_USER:-admin}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}

      # Agent settings
      - MAX_FEEDBACK_ITERATIONS=${MAX_FEEDBACK_ITERATIONS:-10}

      # Search settings
      - SEARCH_DEFAULT_DAYS_BACK=${SEARCH_DEFAULT_DAYS_BACK:-7}
      - SEARCH_DEFAULT_LIMIT=${SEARCH_DEFAULT_LIMIT:-5}

      # Feature flags
      - ENABLE_BACKGROUND_EMBEDDING=${ENABLE_BACKGROUND_EMBEDDING:-true}
      - ENABLE_OPENSEARCH_SYNC=${ENABLE_OPENSEARCH_SYNC:-true}
      - ENABLE_REDIS_CACHE=${ENABLE_REDIS_CACHE:-true}
    ports:
      - "${FASTAPI_PORT:-8000}:8000"
    volumes:
      - ./src:/app/src
      - ./alembic:/app/alembic
    networks:
      - app-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Gradio UI
  # ---------------------------------------------------------------------------
  gradio:
    build:
      context: .
      dockerfile: ui/Dockerfile
    container_name: multi-agentic-gradio
    environment:
      - FASTAPI_ENDPOINT=http://fastapi:8000
      - AIRFLOW_URL=http://airflow-webserver:8080
      - AIRFLOW_ADMIN_USER=${AIRFLOW_ADMIN_USER:-admin}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_INPUT_PREFIX=${S3_INPUT_PREFIX:-input/}
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - GRADIO_METADATA_REFRESH_INTERVAL=${GRADIO_METADATA_REFRESH_INTERVAL:-5}
    ports:
      - "${GRADIO_PORT:-7860}:7860"
    volumes:
      - ./ui:/app/ui
    networks:
      - app-network
    depends_on:
      fastapi:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7860 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
networks:
  app-network:
    driver: bridge
    name: agentic-network

# =============================================================================
# Volumes
# =============================================================================
volumes:
  postgres_data:
    driver: local
    name: agentic-postgres-data

  opensearch_data:
    driver: local
    name: agentic-opensearch-data

  redis_data:
    driver: local
    name: agentic-redis-data
